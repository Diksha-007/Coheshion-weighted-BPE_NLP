# Pretokenization and Tokenization of Indian Languages  
### Using Model-Guided Pretokenization + Cohesion-Weighted BPE

This project explores tokenization strategies tailored for Indian languages, focusing on combining **model-driven pretokenization signals** with a modified **Cohesion-Weighted BPE** algorithm. The goal is to preserve the linguistic structure of Indic scripts and improve downstream tasks like **Machine Translation (MT)**.

---

## ðŸ”¥ Motivation

Standard BPE works well for English but performs poorly for Indian languages because:

- Indic scripts are **akshara-based**, not character-based.  
- Words contain **matras, halant, conjunct consonants**, and rich morphology.  
- Vanilla BPE often **breaks meaningful akshara units**.  
- Token fragmentation directly hurts **translation quality**.

This project designs a tokenization pipeline that respects Indian linguistic structure.

---

## ðŸ§  Overview of Our Approach

We perform two major steps:

### **1. Model-Based Pretokenization**
We train multiple NLP models on Indic datasets and extract **characterwise accuracies** to understand:

- which character combinations are stable,
- which pairs should be merged,
- where natural boundaries should exist.

### **2. Cohesion-Weighted BPE + Machine Translation**
Using model signals + Unicode rules, we create cohesive pretokenized units and apply a modified BPE:


Score(A,B) = Frequency(A,B) * W(A,B)

where the weight **W(A, B)** boosts or penalizes merges based on character categories.

Finally, we evaluate these tokenizers using a **Sanskrit â†’ English** MT task.

---

## ðŸ—ï¸ Pipeline Diagram

Below is the horizontal workflow used in this project:

![Pipeline Diagram](A_flowchart_diagram_in_the_image_illustrates_the_p.png)

---

## ðŸ“š Datasets

We experiment with three datasets:

| Dataset | Description |
|--------|-------------|
| **Sanskrit** | Monolingual and parallel Sanskrit corpora |
| **Hindi** | Standard Hindi monolingual corpus |
| **Mixed (Sanskrit + Hindi)** | Combined dataset to evaluate multilingual generalization |

---

## ðŸ§© Step 1: Training Models to Extract Character Accuracies

We trained the following models:

- **IndicBERT v2** (encoder-only)
- **ByT5-small** (byte-level transformer)
- **BiLSTM character model**

From each model, we collected:

- Character prediction accuracy  
- Confusion matrix  
- Misclassification hotspots  
- Stable character-group patterns  

These accuracies were later used to infer **which characters should stay together** during tokenization.

---

## ðŸ§© Step 2: Model-Guided Pretokenization

Using character-level accuracy:

### **Boost weights (W > 1.0)**
- Consonant + Matra  
- Consonant + Halant  

These combinations form valid **aksharas** and must not be split.

### **Penalty weights (W < 1.0)**
- Vowel â†’ Consonant transitions  
- Boundaries likely representing syllable breaks  

This prevents merging across natural linguistic boundaries.

---

## ðŸ§© Step 3: Cohesion-Weighted BPE

Modified BPE scoring:


**Why this works:**
- Encourages linguistically-valid merges  
- Reduces harmful splits  
- Maintains akshara structure  
- Makes tokenization consistent across models and languages  

---

## ðŸ§© Step 4: Machine Translation Evaluation

We test tokenizers via:

### **Task**
Sanskrit â†’ English translation

### **Models**
- Seq2Seq Transformer

### **Metrics**
- BLEU  


---

## ðŸ“Š Results (Example Structure)

### **BLEU Score Comparison**
| Tokenizer | BLEU â†‘ |
|----------|--------|
| Vanilla BPE | 14 |
| Unicode Pretokenization + BPE | 69 |
| **Cohesion-Weighted BPE (Ours)** | **18** |



---

## ðŸ“‚ Project Structure
```
NLP_2025/
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ machinetranslation/
â”‚   â”‚   â”œâ”€â”€ mix_test.csv
â”‚   â”‚   â”œâ”€â”€ mix_train.csv
â”‚   â”‚   â””â”€â”€ mix_val.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ pretok/
â”‚   â”‚   â”œâ”€â”€ hindi/
â”‚   â”‚   â”‚   â”œâ”€â”€ hindi_test.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ hindi_train.csv
â”‚   â”‚   â”‚   â””â”€â”€ hindi_val.csv
â”‚   â”‚   â”œâ”€â”€ mix/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_mixed.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ train_mixed.csv
â”‚   â”‚   â”‚   â””â”€â”€ val_mixed.csv
â”‚   â”‚   â””â”€â”€ sanskrit/
â”‚   â”‚       â”œâ”€â”€ test.csv
â”‚   â”‚       â”œâ”€â”€ train.csv
â”‚   â”‚       â””â”€â”€ val.csv
â”‚   â”‚
â”‚   â””â”€â”€ tok/
â”‚       â””â”€â”€ token_train.csv
â”‚
â”œâ”€â”€ machine translation/
â”‚   â”œâ”€â”€ cohesive/
â”‚   â”‚   â””â”€â”€ output/
â”‚   â”‚       â”œâ”€â”€ merges.txt_1 â€¦ merges.txt_5
â”‚   â”‚       â”œâ”€â”€ tokenizer.model_1 â€¦ tokenizer.model_5
â”‚   â”‚       â””â”€â”€ tokenizer.vocab_1 â€¦ tokenizer.vocab_5
â”‚   â”‚
â”‚   â”œâ”€â”€ output_10/
â”‚   â”‚   â”œâ”€â”€ merges.txt
â”‚   â”‚   â”œâ”€â”€ tokenizer.model
â”‚   â”‚   â””â”€â”€ tokenizer.vocab
â”‚   â”‚
â”‚   â”œâ”€â”€ mt_normal.py
â”‚   â”œâ”€â”€ mt_pretokenization.py
â”‚
â”‚   â”œâ”€â”€ standard bpe/
â”‚   â”‚   â”œâ”€â”€ bpe/
â”‚   â”‚   â”œâ”€â”€ mt_bpe/
â”‚   â”‚   â”œâ”€â”€ seq2seq_out/
â”‚   â”‚   â”œâ”€â”€ seq2seq_out_no_sandhi/
â”‚   â”‚   â”œâ”€â”€ bpe_mixed.model
â”‚   â”‚   â”œâ”€â”€ bpe_mixed.vocab
â”‚   â”‚   â”œâ”€â”€ mt_bpe_normal.py
â”‚   â”‚   â”œâ”€â”€ mt_bpe_withoutpretok.py
â”‚   â”‚   â””â”€â”€ mt_bpe.py
â”‚
â”‚   â”œâ”€â”€ unigram/
â”‚   â”‚   â”œâ”€â”€ mt_unigram/
â”‚   â”‚   â”œâ”€â”€ seq2seq_out/
â”‚   â”‚   â”œâ”€â”€ seq2seq_out_no_sandhi/
â”‚   â”‚   â”œâ”€â”€ seq2seq_out_norm/
â”‚   â”‚   â”œâ”€â”€ mt_normalization_uni.py
â”‚   â”‚   â”œâ”€â”€ mt_withoutpre_uni.py
â”‚   â”‚   â”œâ”€â”€ mt_withpre_uni.py
â”‚   â”‚   â””â”€â”€ unigram_mixed.vocab
â”‚
â”œâ”€â”€ pretokenization/
â”‚   â”œâ”€â”€ bilstm_attention_model/
â”‚   â”œâ”€â”€ byt5_best_model_hindi/
â”‚   â”œâ”€â”€ byt5_best_model_mixed/
â”‚   â”œâ”€â”€ byt5_best_model_sanskrit/
â”‚   â”œâ”€â”€ byt5_env/
â”‚   â”œâ”€â”€ best_sandhi_model_hindi.pt
â”‚   â”œâ”€â”€ best_sandhi_model_mixed.pt
â”‚   â”œâ”€â”€ best_sandhi_model_sanskrit.pt
â”‚   â”œâ”€â”€ bilstm.py
â”‚   â”œâ”€â”€ byt5_hindi.py
â”‚   â”œâ”€â”€ byt5_mixed.py
â”‚   â”œâ”€â”€ byt5_sanskrit.py
â”‚   â”œâ”€â”€ indictbert_hindi.py
â”‚   â”œâ”€â”€ indictbert_mixed.py
â”‚   â”œâ”€â”€ indictbert_sanskrit.py
â”‚   â”œâ”€â”€ indictbert_new.py
â”‚   â””â”€â”€ test_predictions.csv
â”‚
â”œâ”€â”€ Tokenization/
â”‚   â”œâ”€â”€ sentencepiece_to_huggingface.py
â”‚   â”œâ”€â”€ train_bpe.py
â”‚   â”œâ”€â”€ train_tokenizer_cohesive.py
â”‚   â””â”€â”€ train_unigram.py
â”‚
â””â”€â”€ README.md

```




---

# ðŸš€ Instructions to Run the Project

Below are all the commands to execute the full pipeline.

---


# 1ï¸âƒ£ Pretokenization

## **IndicBERT-v2**
```bash
python3 indicbert.py
```
## **ByT5-Small**
```bash
python3 byt5-small.py
```

## **BiLSTM Character Model**
```bash
python3 bilstm.py
```


# **2ï¸âƒ£ Tokenization**

## **Unigram Model**
```bash
python3 train_uni.py
```

## **Standard BPE**
```bash
python3 train_bpe.py
```
## **Cohesive BPE (Proposed Model)**
```bash
python3 train_tokenizer.py
```




# **3ï¸âƒ£ Machine Translation Experiments**

We evaluated tokenizers for:
âœ” Normalised text
âœ” Without pretoke
âœ” With pretoke

Separated for Unigram, Standard BPE, and Cohesive BPE.

## ðŸŒ Machine Translation â€” Unigram
### Normalised
```bash
python3 mt_normalisation_uni.py
```

### Without Pretokenization
```bash
python3 mt_withoutpre_uni.py
```

### With Pretokenization
```bash
python3 mt_withpre_uni.py
```

## ðŸ“˜ Machine Translation â€” Standard BPE
### Normalised
```bash
python3 mt_normalisation_bpe.py
```

### Without Pretokenization
```bash
python3 mt_withoutpre_bpe.py
```

### With Pretokenization
```bash
python3 mt_withpre_bpe.py
```

## ðŸŸ£ Machine Translation â€” Cohesive BPE (Proposed Method)
### Normalised
```bash
python3 mt_normalisation_cohesive_bpe.py
```

### Without Pretokenization
```bash
python3 mt_withoutpre_cohesive_bpe.py
```

### With Pretokenization
```bash
python3 mt_withpre_cohesive_bpe.py
```



# ðŸ”® Future Work

Extend to more Indic languages (Marathi, Tamil, Telugu, Odia)

Explore neural morphological analyzers for dynamic cohesion scoring

Evaluate on ASR, QA, summarization, and LLM finetuning tasks

# âœ¨ Contributors
```
Riddhima Goyal
Diksha Sharma
Ishika Agarwal
Mehak
Zainab
Manya Gupta
```

---
